start hdfs:
start-dfs.sh
start-yarn.sh


verify service:
jps


pyspark versio:
pip install pyspark==3.3.4



hdfs dfs -copyToLocal /user/hadoop/amazon/enriched_categories/Game_Hardware/Top100_seller/part-00000-9683890d-a4a8-40f2-a001-9e7d534993eb-c000.csv /home/hadoop/Bureau/

hdfs dfs -copyToLocal /user/hadoop/amazon/enriched_categories/Golf_Equipment/Top100_seller/part-00000-68a2d22e-0d2e-4b1e-994e-cecbe79a0a52-c000.csv /home/hadoop/Bureau/



check port:
netstat -tuln | grep 9000


check if hdfs is working:
hdfs dfs -ls /

create directory in hdfs:
hdfs dfs -mkdir /user/hadoop
hdfs dfs -mkdir /user/hadoop/summoner_details

verify dir hdfs:
hdfs dfs -ls /user/hadoop/lol/raw

api key:
RGAPI-09292aea-f9cb-4022-b22c-0b3c5271ba64

git hub auth key: ajouter 7

git config --global credential.helper 'cache --timeout=36000'
ghp_UM4icOJAZ2oTBvFdYzCFIyUNovUcwf3xVBu





airflow install:

faut bien mettre le airflow home en place apres l'install avant le db init ( en permanenet avec nano du bash et le reset avec source et cahnger de terminal)

pip install apache-airflow==2.8.0


python -m site --user-base

export PATH=$PATH:/home/hadoop/.local/bin

echo 'export PATH=$PATH:/home/hadoop/.local/bin' >> ~/.bashrc

source ~/.bashrc
nano ~/.bashrc

airflow --version

export AIRFLOW_HOME=/home/hadoop/Desktop/projectBD/airflow
export AIRFLOW_HOME=/home/hadoop/Desktop/projectBD/airflow

export PYTHONPATH=$PYTHONPATH:/home/hadoop/.local/lib/python3.10/site-packages/pyspark




airflow db init

airflow users create \
  --username admin \
  --firstname Kayu \
  --lastname Lowy \
  --role Admin \
  --email irachide1@gmail.com

pass:123456


airflow start:

airflow webserver -p 8080

airflow scheduler


ps aux | grep '[a]irflow' | awk '{print $2}' | xargs kill -9
airflow db reset
sudo lsof -i :8793 | awk 'NR!=1 {print $2}' | xargs kill -9



find ~ -name "*airflow*"
rm -rf ~/airflow
pip uninstall apache-airflow




/home/hadoop/Desktop/projectBD/
│
├── data/
│   └── summoner_names.json
│
├── scripts/
│   └── fetch_summoner_ids.py
│
├── hadoop/
│   └── (Hadoop configuration files or scripts)
│
├── airflow/
│   └── dags
│        └── summoner_id_dag.py   
│
├── spark/
│   └── (Spark scripts)
│
├── logs/
│   └── (Log files, if any)
│
├── docs/
│   └── (Documentation files)
│
├── info.txt
└── requirements.txt

sudo systemctl start elasticsearch
sudo systemctl start kibana

sudo systemctl stop elasticsearch
sudo systemctl stop kibana


test load:
curl -X GET "localhost:9200/summoner_game_histories/_search?pretty"
